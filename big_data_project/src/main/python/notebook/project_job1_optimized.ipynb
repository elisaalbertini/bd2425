{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": "# Big Data Project"
  },
  {
   "cell_type": "code",
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:18.299735Z",
     "start_time": "2025-03-21T10:05:09.458453Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4046\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1742551512676)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:20.580190Z",
     "start_time": "2025-03-21T10:05:20.049984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "/**\n",
    " * This method creates an RDD with rows [asin, reviewText, overall, category, summary]\n",
    " * given the path of a csv file\n",
    " * */\n",
    "def create(path: String, spark: SparkSession):  RDD[(String, String, Double, String, String)] = {\n",
    "  spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(path).rdd\n",
    "    .map(row => {\n",
    "      val asin = row.getAs[String](\"asin\")\n",
    "      val reviewText = row.getAs[String](\"reviewText\")\n",
    "      val overall = try {\n",
    "        row.getAs[String](\"overall\").toDouble\n",
    "      } catch {\n",
    "        case e: Exception => 0.0\n",
    "      }\n",
    "      val category = row.getAs[String](\"category\")\n",
    "      val summary = row.getAs[String](\"summary\")\n",
    "\n",
    "      (asin, reviewText, overall, category, summary)\n",
    "    })}"
   ],
   "id": "9654f6a432a0a300",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "create: (path: String, spark: org.apache.spark.sql.SparkSession)org.apache.spark.rdd.RDD[(String, String, Double, String, String)]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:28.580434Z",
     "start_time": "2025-03-21T10:05:28.305858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/**\n",
    " * This method cleans a string substituting all the special characters except from ' and\n",
    " * the multiple blank spaces with a blank space. It also trims the string.\n",
    " * */\n",
    "def cleanString(s: String): String = {\n",
    "  s.toLowerCase()\n",
    "    .replaceAll(\"[^a-zA-z0-9 ']\", \" \")\n",
    "    .replaceAll(\"\\\\[\", \" \")\n",
    "    .replaceAll(\"\\\\]\", \" \")\n",
    "    .replaceAll(\"\\\\s+\", \" \")\n",
    "    .trim()\n",
    "}\n",
    "\n",
    "def classifyRating(rating: Double): String = {\n",
    "  rating match {\n",
    "    case r if r <= 2 => \"low rating\"\n",
    "    case r if r == 3 => \"medium rating\"\n",
    "    case _ => \"high rating\"\n",
    "  }\n",
    "}"
   ],
   "id": "88cbdc01b68a0146",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanString: (s: String)String\r\n",
       "classifyRating: (rating: Double)String\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:36.544225Z",
     "start_time": "2025-03-21T10:05:33.060895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val sparkSession: SparkSession = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// create Rdd from csv files\n",
    "val rddReviewAppliances = create(\"../../../../dataset/Appliances_5_part0.csv\", sparkSession)\n",
    "val rddReviewSoftware = create(\"../../../../dataset/Software_5_part0.csv\", sparkSession)\n",
    "\n",
    "sparkSession.sparkContext.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val p = new HashPartitioner(36)\n",
    "\n",
    "// union of the three Rdds\n",
    "val rddUnion = rddReviewAppliances\n",
    "  .union(rddReviewSoftware)"
   ],
   "id": "ee79f8804ccf1405",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2906d57b\r\n",
       "rddReviewAppliances: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[9] at map at <console>:38\r\n",
       "rddReviewSoftware: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[19] at map at <console>:38\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@24\r\n",
       "rddUnion: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = UnionRDD[20] at union at <console>:42\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:38.824698Z",
     "start_time": "2025-03-21T10:05:38.127985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val categoryRatingReviewWords =\n",
    "  rddUnion\n",
    "    // map category as key, remove summary and id, clean review string and replace rating with class of rating\n",
    "    .map({case (id, review, rating, category, summary) => (category, (cleanString(review), classifyRating(rating)))})\n",
    "    // remove rows where the cleaned string is empty\n",
    "    .filter(x =>  x._2._1 != \"\")\n",
    "    // replace review with the number of words in it\n",
    "    .map({case (category, (review, rating)) => (category, (rating, review.split(\" \").length))})\n",
    "    // optimization\n",
    "    .partitionBy(p)\n",
    "    .persist(MEMORY_AND_DISK)"
   ],
   "id": "37055f829088f298",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryRatingReviewWords: org.apache.spark.rdd.RDD[(String, (String, Int))] = ShuffledRDD[24] at partitionBy at <console>:41\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:42.163236Z",
     "start_time": "2025-03-21T10:05:41.799865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val totAllWordPerCategory =\n",
    "  categoryRatingReviewWords\n",
    "    // map category as key and the number of words as value while keeping the partitioning\n",
    "    .mapValues(x => x._2)\n",
    "    // compute the total number of words for each category, adding the values\n",
    "    .aggregateByKey(0.0)((a, l) => a + l, (a1, a2) => a1 + a2)\n",
    "    //optimization\n",
    "    .partitionBy(p)\n",
    "    .persist(MEMORY_AND_DISK)"
   ],
   "id": "e085a06c248d6e9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totAllWordPerCategory: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[26] at aggregateByKey at <console>:35\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:45.953537Z",
     "start_time": "2025-03-21T10:05:45.544934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// first we join these two Rdds to add the total number of words in each category\n",
    "val wordFreqSubCategory =\n",
    "  categoryRatingReviewWords\n",
    "    .join(totAllWordPerCategory)\n",
    "    // adding category, rating class and the total number of words per category to the key.\n",
    "    // We need this in order to compute the number of words for each different key while keeping the number of words per category.\n",
    "    .map({case (category, ((classification, words), allWords)) => ((category, classification, allWords), words)})\n",
    "    .reduceByKey(_ + _)\n",
    "    // map to compute the ratio between the number of words for each class of rating and the total number of words for each category\n",
    "    // map to write as DF on file\n",
    "    .map({case ((category, classification, allWords), words) =>\n",
    "      (category, classification, words/allWords)})"
   ],
   "id": "3ff173fc17ac6724",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordFreqSubCategory: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[32] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:05:50.723992Z",
     "start_time": "2025-03-21T10:05:47.638289Z"
    }
   },
   "cell_type": "code",
   "source": "wordFreqSubCategory.collect()",
   "id": "3984f5828eb4e81a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, String, Double)] = Array((Appliances_5,high rating,0.7444507148231754), (Appliances_5,medium rating,0.09621896162528217), (Software_5,high rating,0.6494401176760477), (Software_5,medium rating,0.16578866283296856), (Appliances_5,low rating,0.1593303235515425), (Software_5,low rating,0.18477121949098377))\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
