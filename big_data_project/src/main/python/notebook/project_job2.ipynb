{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": "# Big Data Project - Job 2"
  },
  {
   "cell_type": "code",
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:58:29.102537Z",
     "start_time": "2025-03-21T09:58:16.704829Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4044\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1742551102583)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:58:33.854310Z",
     "start_time": "2025-03-21T09:58:33.293117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sparkSession = SparkSession.builder().getOrCreate()\n",
    "\n",
    "/**\n",
    " * This method creates an RDD with rows [asin, reviewText, overall, category, summary]\n",
    " * given the path of a csv file\n",
    " * */\n",
    "def create(path: String, spark: SparkSession):  RDD[(String, String, Double, String, String)] = {\n",
    "  spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(path).rdd\n",
    "    .map(row => {\n",
    "      val asin = row.getAs[String](\"asin\")\n",
    "      val reviewText = row.getAs[String](\"reviewText\")\n",
    "      val overall = try {\n",
    "        row.getAs[String](\"overall\").toDouble\n",
    "      } catch {\n",
    "        case e: Exception => 0.0\n",
    "      }\n",
    "      val category = row.getAs[String](\"category\")\n",
    "      val summary = row.getAs[String](\"summary\")\n",
    "\n",
    "      (asin, reviewText, overall, category, summary)\n",
    "    })}"
   ],
   "id": "9654f6a432a0a300",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@78c1286c\r\n",
       "create: (path: String, spark: org.apache.spark.sql.SparkSession)org.apache.spark.rdd.RDD[(String, String, Double, String, String)]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:58:44.848722Z",
     "start_time": "2025-03-21T09:58:44.618958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/**\n",
    " * This method cleans a string substituting all the special characters except from ' and\n",
    " * the multiple blank spaces with a blank space. It also trims the string.\n",
    " * */\n",
    "def cleanString(s: String): String = {\n",
    "  s.toLowerCase()\n",
    "    .replaceAll(\"[^a-zA-z0-9 ']\", \" \")\n",
    "    .replaceAll(\"\\\\[\", \" \")\n",
    "    .replaceAll(\"\\\\]\", \" \")\n",
    "    .replaceAll(\"\\\\s+\", \" \")\n",
    "    .trim()\n",
    "}"
   ],
   "id": "88cbdc01b68a0146",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanString: (s: String)String\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:58:51.411267Z",
     "start_time": "2025-03-21T09:58:47.545184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val rddReviewAppliances = create(\"../../../../dataset/Appliances_5_part0.csv\", sparkSession)\n",
    "val rddReviewSoftware = create(\"../../../../dataset/Software_5_part0.csv\", sparkSession)\n",
    "\n",
    "// union of the three Rdds\n",
    "val rddUnion = rddReviewAppliances\n",
    "  .union(rddReviewSoftware)\n",
    "  // remove review and category column, clean summary string\n",
    "  .map({case (id, review, rating, category, summary) => (id, rating, cleanString(summary))})\n",
    "  // remove rows where the cleaned string is empty\n",
    "  .filter(x => x._3 != \"\")\n",
    "  // map id as key, replace summary with the words\n",
    "  .map(x => (x._1, (x._2, x._3.split(\" \"))))"
   ],
   "id": "ee79f8804ccf1405",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddReviewAppliances: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[9] at map at <console>:41\r\n",
       "rddReviewSoftware: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[19] at map at <console>:41\r\n",
       "rddUnion: org.apache.spark.rdd.RDD[(String, (Double, Array[String]))] = MapPartitionsRDD[23] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:58:59.904456Z",
     "start_time": "2025-03-21T09:58:59.428947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// create filter based on rating and number of reviews\n",
    "val filter = rddUnion\n",
    "    // remove words column\n",
    "    .map({case (id, (rating, words)) => (id, rating)})\n",
    "    // aggregate by id to compute the sum of all the ratings for the product and count the number of reviews\n",
    "    .aggregateByKey((0.0, 0.0))((a, r) => (a._1 + r, a._2 + 1), (a1, a2) => (a1._1 + a2._1, a1._2 + a2._2))\n",
    "    // compute the average rating, keep the number of reviews\n",
    "    .map({case (id, (ratingSum, ratingNumber)) => (id, (ratingSum/ratingNumber, ratingNumber))})\n",
    "    // filter to keep products with high avg rating but few reviews\n",
    "    .filter(x => x._2._1 >= 4 && x._2._2 < 10)"
   ],
   "id": "6ee256ca08bd1a60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter: org.apache.spark.rdd.RDD[(String, (Double, Double))] = MapPartitionsRDD[27] at filter at <console>:36\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:59:12.491397Z",
     "start_time": "2025-03-21T09:59:12.217197Z"
    }
   },
   "cell_type": "code",
   "source": [
    " val filteredRdd = rddUnion\n",
    "  // remove rating column\n",
    "  .map({case (id, (rating, words)) => (id, words)})\n",
    "  // join used to filter the original Rdd\n",
    "  .join(filter)"
   ],
   "id": "c893f683212986a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filteredRdd: org.apache.spark.rdd.RDD[(String, (Array[String], (Double, Double)))] = MapPartitionsRDD[31] at join at <console>:32\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:59:19.515077Z",
     "start_time": "2025-03-21T09:59:19.169842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// now we work with the filter Rdd to count how many times each single word appears in all the summaries\n",
    "val wordFreq = filteredRdd\n",
    "    // flat map is used to create a Rdd where each tuple consist of a word and the number 1\n",
    "    .flatMap({case (_, (summary, _))=>\n",
    "      summary\n",
    "        .map(x=>(x, 1.0))})\n",
    "    // now it is possible to count the occurrences of each word\n",
    "    .reduceByKey(_+_)"
   ],
   "id": "26f3e22f58e13a7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordFreq: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[33] at reduceByKey at <console>:34\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:59:58.533968Z",
     "start_time": "2025-03-21T09:59:57.424968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// now we work with the filter Rdd to count how many times each single word appears in the summaries of a single product\n",
    "val wordFreqPerReview = filteredRdd\n",
    "    .flatMap({case (id, (summary, _))=>\n",
    "      summary\n",
    "        .map(x=>((id, x), 1.0))})\n",
    "    .reduceByKey(_ + _)\n",
    "    // map because we need the words as key for the join\n",
    "    .map({case ((id, word), count) => (word, (id, count))})\n",
    "    // join to obtain the total frequency of each word\n",
    "    .join(wordFreq)\n",
    "    // compute the ratio between the frequency of the word in the reviews of a product and the total frequency\n",
    "    .map({case (word, ((id, count), tot)) => (word, (id, count/tot))})\n",
    "    // we use mapValues to place the values inside a List, we do this so that, later, it is possible to\n",
    "    // put together a list of all the products the word appears in, together with the ratio computed in the previous map\n",
    "    .mapValues(x => List(x))\n",
    "    // we create the list appending the values for the same word (key)\n",
    "    .reduceByKey((a, b) => a ++ b)\n",
    "    // map to change the key to the number of products a word appears in, and sort based on it\n",
    "    .map(x => (x._2.size, (x._1, x._2)))\n",
    "    .sortByKey(ascending = false)\n",
    "    // we keep only the words that appear in multiple products\n",
    "    .filter(x => x._1 > 10)\n",
    "    // map to write as DF on file\n",
    "    .map(x => (x._1, x._2._1, x._2._2.toString()))"
   ],
   "id": "a1647f06fd878f40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordFreqPerReview: org.apache.spark.rdd.RDD[(Int, String, String)] = MapPartitionsRDD[63] at map at <console>:51\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:00:00.486277Z",
     "start_time": "2025-03-21T10:00:00.148313Z"
    }
   },
   "cell_type": "code",
   "source": "wordFreqPerReview.collect()",
   "id": "7e9c2c1def3e47a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(Int, String, String)] = Array((89,stars,List((B0056I99WG,0.020202020202020204), (B00MGMWTQS,0.020202020202020204), (B009CCVMOU,0.005050505050505051), (B009APQH76,0.005050505050505051), (B000B8K7SG,0.005050505050505051), (B00P9C66PM,0.025252525252525252), (B004XLDE5A,0.025252525252525252), (B00E6LIEFM,0.010101010101010102), (B00GRFIIHO,0.015151515151515152), (B008YDSH6E,0.010101010101010102), (B004E9SKE6,0.005050505050505051), (B00W8DIFCM,0.025252525252525252), (B004E564PW,0.005050505050505051), (B00E6OPDU8,0.005050505050505051), (B004I49NJ8,0.005050505050505051), (B00DM8KQ2Y,0.015151515151515152), (B007A7JSMM,0.005050505050505051), (B0053F7TQA,0.015151515151515152), (B01637RLIW,0.005050505050505051), (B002SR0QDO,0.010101010101010102), (B00MUY6KY4,0.005050505050505051), (B00...\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
