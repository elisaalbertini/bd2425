{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": "# Big Data Project - Job 2 Optimized"
  },
  {
   "cell_type": "code",
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:03:14.446302Z",
     "start_time": "2025-03-21T10:03:05.272816Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4045\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1742551388696)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:03:18.134138Z",
     "start_time": "2025-03-21T10:03:17.475502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sparkSession = SparkSession.builder().getOrCreate()\n",
    "\n",
    "/**\n",
    " * This method creates an RDD with rows [asin, reviewText, overall, category, summary]\n",
    " * given the path of a csv file\n",
    " * */\n",
    "def create(path: String, spark: SparkSession):  RDD[(String, String, Double, String, String)] = {\n",
    "  spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(path).rdd\n",
    "    .map(row => {\n",
    "      val asin = row.getAs[String](\"asin\")\n",
    "      val reviewText = row.getAs[String](\"reviewText\")\n",
    "      val overall = try {\n",
    "        row.getAs[String](\"overall\").toDouble\n",
    "      } catch {\n",
    "        case e: Exception => 0.0\n",
    "      }\n",
    "      val category = row.getAs[String](\"category\")\n",
    "      val summary = row.getAs[String](\"summary\")\n",
    "\n",
    "      (asin, reviewText, overall, category, summary)\n",
    "    })}"
   ],
   "id": "9654f6a432a0a300",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@253cb773\r\n",
       "create: (path: String, spark: org.apache.spark.sql.SparkSession)org.apache.spark.rdd.RDD[(String, String, Double, String, String)]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:03:27.188137Z",
     "start_time": "2025-03-21T10:03:26.988396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/**\n",
    " * This method cleans a string substituting all the special characters except from ' and\n",
    " * the multiple blank spaces with a blank space. It also trims the string.\n",
    " * */\n",
    "def cleanString(s: String): String = {\n",
    "  s.toLowerCase()\n",
    "    .replaceAll(\"[^a-zA-z0-9 ']\", \" \")\n",
    "    .replaceAll(\"\\\\[\", \" \")\n",
    "    .replaceAll(\"\\\\]\", \" \")\n",
    "    .replaceAll(\"\\\\s+\", \" \")\n",
    "    .trim()\n",
    "}"
   ],
   "id": "88cbdc01b68a0146",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanString: (s: String)String\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:03:31.936907Z",
     "start_time": "2025-03-21T10:03:28.639085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK\n",
    "\n",
    "val rddReviewAppliances = create(\"../../../../dataset/Appliances_5_part0.csv\", sparkSession)\n",
    "val rddReviewSoftware = create(\"../../../../dataset/Software_5_part0.csv\", sparkSession)\n",
    "\n",
    "val p = new HashPartitioner(36)\n",
    "\n",
    "// union of the three Rdds\n",
    "val rddUnion = rddReviewAppliances\n",
    "  .union(rddReviewSoftware)\n",
    "  // remove review and category column, clean summary string\n",
    "  .map({case (id, review, rating, category, summary) => (id, rating, cleanString(summary))})\n",
    "  // remove rows where the cleaned string is empty\n",
    "  .filter(x => x._3 != \"\")\n",
    "  // map id as key, replace summary with the words\n",
    "  .map(x => (x._1, (x._2, x._3.split(\" \"))))"
   ],
   "id": "ee79f8804ccf1405",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "import org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK\r\n",
       "rddReviewAppliances: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[9] at map at <console>:41\r\n",
       "rddReviewSoftware: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[19] at map at <console>:41\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@24\r\n",
       "rddUnion: org.apache.spark.rdd.RDD[(String, (Double, Array[String]))] = MapPartitionsRDD[23] at map at <console>:45\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:03:36.300815Z",
     "start_time": "2025-03-21T10:03:35.794344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// create filter based on rating and number of reviews\n",
    "val filter = rddUnion\n",
    "    // remove words column\n",
    "    .map({case (id, (rating, words)) => (id, rating)})\n",
    "    // aggregate by id to compute the sum of all the ratings for the product and count the number of reviews\n",
    "    .aggregateByKey((0.0, 0.0))((a, r) => (a._1 + r, a._2 + 1), (a1, a2) => (a1._1 + a2._1, a1._2 + a2._2))\n",
    "    // compute the average rating, keep the number of reviews\n",
    "    .map({case (id, (ratingSum, ratingNumber)) => (id, (ratingSum/ratingNumber, ratingNumber))})\n",
    "    // filter to keep products with high avg rating but few reviews\n",
    "    .filter(x => x._2._1 >= 4 && x._2._2 < 10)"
   ],
   "id": "6ee256ca08bd1a60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter: org.apache.spark.rdd.RDD[(String, (Double, Double))] = MapPartitionsRDD[27] at filter at <console>:38\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:03:38.052147Z",
     "start_time": "2025-03-21T10:03:37.608953Z"
    }
   },
   "cell_type": "code",
   "source": [
    " val filteredRdd = rddUnion\n",
    "  // remove rating column\n",
    "  .map({case (id, (rating, words)) => (id, words)})\n",
    "  // optimization\n",
    "  .partitionBy(p)\n",
    "  // join used to filter the original Rdd\n",
    "  .join(filter)\n",
    "  // optimization\n",
    "  .persist(MEMORY_AND_DISK)"
   ],
   "id": "c893f683212986a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filteredRdd: org.apache.spark.rdd.RDD[(String, (Array[String], (Double, Double)))] = MapPartitionsRDD[32] at join at <console>:37\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:03:43.782633Z",
     "start_time": "2025-03-21T10:03:43.468693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// now we work with the filter Rdd to count how many times each single word appears in all the summaries\n",
    "val wordFreq = filteredRdd\n",
    "    // flat map is used to create a Rdd where each tuple consist of a word and the number 1\n",
    "    .flatMap({case (_, (summary, _))=>\n",
    "      summary\n",
    "        .map(x=>(x, 1.0))})\n",
    "    // now it is possible to count the occurrences of each word\n",
    "    .reduceByKey(_+_)"
   ],
   "id": "26f3e22f58e13a7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordFreq: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[34] at reduceByKey at <console>:36\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:04:16.284893Z",
     "start_time": "2025-03-21T10:04:14.455846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// now we work with the filter Rdd to count how many times each single word appears in the summaries of a single product\n",
    "val wordFreqPerReview = filteredRdd\n",
    "    .flatMap({case (id, (summary, _))=>\n",
    "      summary\n",
    "        .map(x=>((id, x), 1.0))})\n",
    "    .reduceByKey(_ + _)\n",
    "    // map because we need the words as key for the join\n",
    "    .map({case ((id, word), count) => (word, (id, count))})\n",
    "    // join to obtain the total frequency of each word\n",
    "    .join(wordFreq)\n",
    "    // compute the ratio between the frequency of the word in the reviews of a product and the total frequency\n",
    "    .map({case (word, ((id, count), tot)) => (word, (id, count/tot))})\n",
    "    // we use mapValues to place the values inside a List, we do this so that, later, it is possible to\n",
    "    // put together a list of all the products the word appears in, together with the ratio computed in the previous map\n",
    "    .mapValues(x => List(x))\n",
    "    // we create the list appending the values for the same word (key)\n",
    "    .reduceByKey((a, b) => a ++ b)\n",
    "    // map to change the key to the number of products a word appears in, and sort based on it\n",
    "    .map(x => (x._2.size, (x._1, x._2)))\n",
    "    .sortByKey(ascending = false)\n",
    "    // we keep only the words that appear in multiple products\n",
    "    .filter(x => x._1 > 10)\n",
    "    // map to write as DF on file\n",
    "    .map(x => (x._1, x._2._1, x._2._2.toString()))"
   ],
   "id": "a1647f06fd878f40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordFreqPerReview: org.apache.spark.rdd.RDD[(Int, String, String)] = MapPartitionsRDD[64] at map at <console>:53\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T10:04:20.952525Z",
     "start_time": "2025-03-21T10:04:20.310843Z"
    }
   },
   "cell_type": "code",
   "source": "wordFreqPerReview.collect()",
   "id": "7e9c2c1def3e47a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(Int, String, String)] = Array((89,stars,List((B0056I99WG,0.020202020202020204), (B00MGMWTQS,0.020202020202020204), (B00NG7K2RA,0.015151515151515152), (B01637RISK,0.015151515151515152), (B001GL6QDM,0.010101010101010102), (B001DPFP88,0.020202020202020204), (B00GRFIIHO,0.015151515151515152), (B005FDK7J6,0.005050505050505051), (B002U0L1BU,0.005050505050505051), (B00W4YKCGC,0.020202020202020204), (B015IHWAZW,0.025252525252525252), (B00FFINRG6,0.005050505050505051), (B008SCNCTI,0.020202020202020204), (B00PG8FWS6,0.015151515151515152), (B00132DENO,0.005050505050505051), (B004E564PW,0.005050505050505051), (B004I49NJ8,0.005050505050505051), (B0043T8K8I,0.010101010101010102), (B00DM8KQ2Y,0.015151515151515152), (B00E6LI5NI,0.010101010101010102), (B00ENFYLOO,0.005050505050505051), (B00...\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
