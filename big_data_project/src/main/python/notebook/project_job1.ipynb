{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": "# Big Data Project - Job 1"
  },
  {
   "cell_type": "code",
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:06.683900Z",
     "start_time": "2025-03-21T09:43:54.990793Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4041\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1742550239363)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:15.316413Z",
     "start_time": "2025-03-21T09:44:14.590743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "/**\n",
    " * This method creates an RDD with rows [asin, reviewText, overall, category, summary]\n",
    " * given the path of a csv file\n",
    " * */\n",
    "def create(path: String, spark: SparkSession):  RDD[(String, String, Double, String, String)] = {\n",
    "  spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(path).rdd\n",
    "    .map(row => {\n",
    "      val asin = row.getAs[String](\"asin\")\n",
    "      val reviewText = row.getAs[String](\"reviewText\")\n",
    "      val overall = try {\n",
    "        row.getAs[String](\"overall\").toDouble\n",
    "      } catch {\n",
    "        case e: Exception => 0.0\n",
    "      }\n",
    "      val category = row.getAs[String](\"category\")\n",
    "      val summary = row.getAs[String](\"summary\")\n",
    "\n",
    "      (asin, reviewText, overall, category, summary)\n",
    "    })}"
   ],
   "id": "9654f6a432a0a300",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "create: (path: String, spark: org.apache.spark.sql.SparkSession)org.apache.spark.rdd.RDD[(String, String, Double, String, String)]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:19.956165Z",
     "start_time": "2025-03-21T09:44:19.692659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/**\n",
    " * This method cleans a string substituting all the special characters except from ' and\n",
    " * the multiple blank spaces with a blank space. It also trims the string.\n",
    " * */\n",
    "def cleanString(s: String): String = {\n",
    "  s.toLowerCase()\n",
    "    .replaceAll(\"[^a-zA-z0-9 ']\", \" \")\n",
    "    .replaceAll(\"\\\\[\", \" \")\n",
    "    .replaceAll(\"\\\\]\", \" \")\n",
    "    .replaceAll(\"\\\\s+\", \" \")\n",
    "    .trim()\n",
    "}\n",
    "\n",
    "def classifyRating(rating: Double): String = {\n",
    "  rating match {\n",
    "    case r if r <= 2 => \"low rating\"\n",
    "    case r if r == 3 => \"medium rating\"\n",
    "    case _ => \"high rating\"\n",
    "  }\n",
    "}"
   ],
   "id": "88cbdc01b68a0146",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanString: (s: String)String\r\n",
       "classifyRating: (rating: Double)String\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:29.388394Z",
     "start_time": "2025-03-21T09:44:25.900711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val sparkSession: SparkSession = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// create Rdd from csv files\n",
    "val rddReviewAppliances = create(\"../../../../dataset/Appliances_5_part0.csv\", sparkSession)\n",
    "val rddReviewSoftware = create(\"../../../../dataset/Software_5_part0.csv\", sparkSession)\n",
    "\n",
    "sparkSession.sparkContext.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "// union of the three Rdds\n",
    "val rddUnion = rddReviewAppliances\n",
    "  .union(rddReviewSoftware)"
   ],
   "id": "ee79f8804ccf1405",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@602a21a1\r\n",
       "rddReviewAppliances: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[9] at map at <console>:38\r\n",
       "rddReviewSoftware: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = MapPartitionsRDD[19] at map at <console>:38\r\n",
       "rddUnion: org.apache.spark.rdd.RDD[(String, String, Double, String, String)] = UnionRDD[20] at union at <console>:37\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:31.995443Z",
     "start_time": "2025-03-21T09:44:31.339701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val categoryRatingReviewWords =\n",
    "  rddUnion\n",
    "    // map category as key, remove summary and id, clean review string and replace rating with class of rating\n",
    "    .map({case (id, review, rating, category, summary) => (category, (cleanString(review), classifyRating(rating)))})\n",
    "    // remove rows where the cleaned string is empty\n",
    "    .filter(x =>  x._2._1 != \"\")\n",
    "    // replace review with the number of words in it\n",
    "    .map({case (category, (review, rating)) => (category, (rating, review.split(\" \").length))})"
   ],
   "id": "37055f829088f298",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryRatingReviewWords: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[23] at map at <console>:36\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:36.266001Z",
     "start_time": "2025-03-21T09:44:35.817898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val totAllWordPerCategory =\n",
    "  categoryRatingReviewWords\n",
    "    // map category as key and the number of words as value while keeping the partitioning\n",
    "    .mapValues(x => x._2)\n",
    "    // compute the total number of words for each category, adding the values\n",
    "    .aggregateByKey(0.0)((a, l) => a + l, (a1, a2) => a1 + a2)"
   ],
   "id": "e085a06c248d6e9e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "totAllWordPerCategory: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[25] at aggregateByKey at <console>:32\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:40.594049Z",
     "start_time": "2025-03-21T09:44:40.168417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// first we join these two Rdds to add the total number of words in each category\n",
    "val wordFreqSubCategory =\n",
    "  categoryRatingReviewWords\n",
    "    .join(totAllWordPerCategory)\n",
    "    // adding category, rating class and the total number of words per category to the key.\n",
    "    // We need this in order to compute the number of words for each different key while keeping the number of words per category.\n",
    "    .map({case (category, ((classification, words), allWords)) => ((category, classification, allWords), words)})\n",
    "    .reduceByKey(_ + _)\n",
    "    // map to compute the ratio between the number of words for each class of rating and the total number of words for each category\n",
    "    // map to write as DF on file\n",
    "    .map({case ((category, classification, allWords), words) =>\n",
    "      (category, classification, words/allWords)})"
   ],
   "id": "3ff173fc17ac6724",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordFreqSubCategory: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[31] at map at <console>:38\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:44:46.540428Z",
     "start_time": "2025-03-21T09:44:41.945284Z"
    }
   },
   "cell_type": "code",
   "source": "wordFreqSubCategory.collect()",
   "id": "3984f5828eb4e81a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, String, Double)] = Array((Software_5,high rating,0.6494401176760477), (Software_5,low rating,0.18477121949098377), (Software_5,medium rating,0.16578866283296856), (Appliances_5,low rating,0.1593303235515425), (Appliances_5,high rating,0.7444507148231754), (Appliances_5,medium rating,0.09621896162528217))\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
